\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[usenames,dvipsnames]{color}

\title{CS909: Week 10}
\author{1037559}
\date{}

\begin{document}

\maketitle

\section{Introduction}
This report details the implementation of the text classification system required for the week 10 assignment. It describes in detail each stage of the assignment, the reasons for any decisions and the outcomes of these. The first section corresponds to reading in the data, and then feature selection and reduction is discussed. The classification task is then described and evaluated, followed by the clustering task. Finally conclusions are made. Any results can be found in the appendices following the report. The code for the implementation may be found on GitHub, at https://github.com/RuthRainbow/DataMining.

\section{Data Input}
The first task was to read in the data, which is in SGM format. This was done using the \verb|BeautifulSoup| module for Python. This module is able to parse SGM files so data can be extracted by tag, which is useful for this assignment as items such as the topic can easily be extracted. For each data file a \verb|BeautifulSoup| object was created, and these added to a list of ``soups''.

The next task was to extract the relevant tags from the soups. \verb|BeautifulSoup| objects provide methods for extracting tags easily, for example using the methods \verb|find_all| and \verb|findChildren|. \verb|find_all| was used to find all text tags and reuters tags. Within the reuters tags the lewissplit and topic attributes were extracted. Topics were extracted in this way in order to provide a list of topics where an article includes multiple topics.

All extracted entries are then examined. Firstly the body is preprocessed as discussed in the preprocessing section. This is to avoid repeating preprocessing, which is computationally expensive, for bodies with multiple topics. The corresponding topics are then checked to see if they are a topic we are interested in, if so the lewis attribute is checked to find whether to place the text in the training or test lists. The text is added one time for every topic tag it contains that we are interested in. This is so it can be labelled with every relevant label, as each text in the classification may only have one label. All texts are also added once to a separate list in order to be used by the classification algorithms, which look at all data.

The table below shows the frequency of the main topic labels for classification obtained over the 22 SGM files:
\\
\begin{center}
\begin{tabular}{l | l}
Topic & Frequency \\ \hline
earn & 3961 \\
acq & 2367 \\
money-fx & 717 \\
grain & 583 \\
crude & 580 \\
trade & 487 \\
interest & 480 \\
ship & 287 \\
wheat & 283 \\
corn & 239 \\ \hline
Total number of texts: 9984 \\
\end{tabular}
\end{center}

Taking all topics into account TODO topics were present.

\section{Preprocessing}
After the relevant texts had been extracted preprocessing takes place. This is done text by text when extracting text tags as described above using the \verb|preprocess| function. The level of preprocessing depends upon the classification methods being used - if the \verb|scilearn| classifiers are being used these come with their own preprocessing methods so a more similar form of preprocessing is carried out.

For the basic preprocessing firstly the text is changed to utf-8 encoding so all characters can be manipulated by the Python string methods. Secondly the title and date are removed, as these should not be part of the text. Next the word ``Reuter'' is removed from the end of the text - this is present as it is part of the tag for all texts. The text is then converted to lower case and punctuation and numbers removed, using the \verb|regex| module for Python. If \verb|scilearn| is being used no further preprocessing is required here. An example of the effects of this preprocessing can be seen below:

Before preprocessing (raw data):
\\``U.K. MONEY MARKET SHORTAGE FORECAST REVISED DOWN
    LONDON, March 3 - The Bank of England said it had revised
its forecast of the shortage in the money market down to 450
mln stg before taking account of its morning operations. At
noon the bank had estimated the shortfall at 500 mln stg.
 REUTER
''

Post basic preprocessing:
\\``the bank of england said it had revised its forecast of the shortage in the money market down to mln stg before taking account of its morning operations at noon the bank had estimated the shortfall at mln stg''

If \verb|scilearn| is not being used some further preprocessing is required. Firstly Stopwords are removed, using an English corpora as all stopwords should be English. This corpora of stopwords was obtained through the \verb|nltk| module. Next a spell corrector from the \verb|WordNet| module is used and a part of speech tagger applied. After this words are lemmatised in order to obtain the ``stem'' of a word, for example ``runner'' should be reduced to ``run''. This requires the tags to be in a slightly simpler form, so a helper method was included to convert to this form. An English language lemmatiser is then used from the \verb|nltk| module. Optionally a named entity tagger may also be applied to replace things such as company names in the texts. This is optional as it is very computationally expensive and may take a long time. The results of this can be seen in the example below, where it's clear that words have been lemmatised fairly successfully:

``[u'bank', u'england', 'say', u'revise', u'forecast', u'shortage', u'money', u'market', u'man', u'st', u'take', u'account', u'morning', u'operation', u'noon', u'bank', u'estimate', u'shortfall', u'man', u'st']''

\section{Feature Selection}
A number of different feature selection mechanisms were implemented and evaluated to determine the method must suitable for the task. Features are selected as words present or absent in each text.

SHOW RESULTS

\subsection{Bag of words}
Firstly, a `bag of words' approach was implemented, where each text is represented as a vector where every column represents a word in the vocabulary. Entries in the vector indicate the presence or absence of a word. This creates a sparse vector representation of every text.

This approach was implemented in two ways. Firstly, using \verb|nltk| to create a dictionary of all vocabulary and using this to convert each text. This allowed any words which appeared in less than $n$ documents to be filtered out to attempt to reduce the dimensionality of the final vectors.

The second approach used scilearn's \verb|CountVectorizer|. This simply creates feature vectors for each text with a frequency count for each ``feature'' or word in the vocabulary. Optionally this can be used as a binary vectoriser, indicating only the presence or absence of a word. This is also used to remove English stop words and accents. A lemmatiser is also applied through the vectoriser, as guided by part of speech tagging in a similar way to as described above for \verb|TextBlob| preprocessing.

\subsection{Term Frequency - Inverse Document Frequency (TF-IDF)}
This approach attempts to normalise the data and take into account the relative frequencies of words in the text in comparison to the frequency over all texts. This approach provides more information than the previous approach as it also takes into account frequency, which may be important to classification.

The first implementation of this was provided by manually summing the frequencies of all words and applying the formaluae for TF and IDF as defined in the lectures:
\\$tf(t, d) = \frac{Frequency of term t in document d}{max. frequency of t in any doc d}$
\\$idf(t, D) = log\frac{|D|}{df(t,D)}$ where $D$ is the set of all documents. The words with the highest 50 TF-IDF scores were used as features.

The second implementation used the \verb|TfidfVectorizer| from SciLearn. This vectoriser can also be used for some preprocessing steps, such as removing accents and stop words, so only the simpler preprocessing is required for SciLearn. The same lemmatiser is applied as for the \verb|CountVectorizer|. This vectoriser also normalises the features.

\subsection{Feature Reduction}
Feature reduction is lastly applied to all feature selection methods in order to eliminate irrelevant or rare features from the dataset and reduce the dimensionality of the vectors to further focus on more relevant features.

For SciLearn, a $\chi^2$ test is applied to select the k best features. This test determines how relevant words are to determining a class, and the k most relevant features are retained for each class.

TODO show chosen features

\subsubsection{Clustering}
It was found that these feature selection methods still created too sparse a vector for some clustering methods. There is also a greater number of examples for these algorithms to fit as the algorithms are unsupervised so the data is not split into testing and training sets. For this reason a \verb|HashingVectorizer| was used before the vectors listed above so the process for the larger number of texts is more memory scalable. Less features are also retained during the $\chi^2$ test.

To further reduce dimensionality the feature vectors a form of Principle Components Analysis (PCA) is applied. The implementation of this was carried out using \verb|TruncatedSVD| from SciLearn, which is a variant of PCA that is able to work on feature vectors in a memory efficient way. This was required as the implementation of PCA was not able to keep all texts in memory with the full data set. After this the vectors need to be renormalised for the clustering algorithms. 

\subsection{Topic Models}
Topic models were created using the \verb|gensim| module. A ``bag of words'' approach was used as described above. All texts are then converted to vectors, mapping word identifiers to the frequency of the words occurring in that text. These vectors can then be used to create topic models, which give probability distributions for each text appearing in a given number of topics. The models were created using 10 topics, as we know this is the number present from the preprocessing.

Firstly a Latent Dirichlet Allocation (LDA) model was created, as described in the lectures. This was created by passing the dictionary and the vectors into an \verb|LdaModel| object, from the \verb|gensim| module. Here are the results obtained from the first three texts:

TODO

\section{Classifier Design \& Implementation}
Classification was carried out using the features selected with a variety of different classifiers. For each a number of metrics were collected in order to effectively compare the methods. A variety of different classifiers were applied in order to compare their effectiveness at classifying the texts into topics. Training and test data sets were obtained using the lewissplit tag as described in the data input section.

\subsection{TextBlob module}
The \verb|TextBlob| module contains some simple classifier implementations which were experimented with first. These classifiers use their own simple binary feature selection method which indicates whether words are present or absent in each text. Two classifier implementations were used; firstly a Na{\"i}ve Bayes classifier and secondly a decision tree classifier. However not a lot of metrics are easily available for these classes so only accuracy was compared. These classifiers are interesting as they allow you to view the most important features they used for each class, however due to the lack of metrics the main classification was done using the SciLearn model, as described below.

\subsection{SciLearn module}
The SciLearn module provides a much greater variety of classifiers which are more customisable with a large number of optional flags. The output of these is also much more easily obtainable, with methods to provide metrics for the classifier and for each class.

To make the code more readable a \verb|classify| method was written which takes in a classifier class as a parameter. This fits the classifier to the training data and uses this to predict labels for the test data. It then prints the metrics as discussed below.

Each classifier is trained using 10-fold cross validation, obtaining splits from the \verb|KFold| class. This aims to obtain independent estimates of how well the classifier will perform on unseen data. The method shuffles the data and tells you the indexes from which each fold should be taken. Each classifier is ran over this and the accuracy metrics for each fold recorded. From the 10 folds the mean and standard deviation of the accuracy is calculated using \verb|numpy|. These are used to calculate 95\% confidence intervals for the accuracy, assuming each follows a normal distribution. There are enough training examples for the distribution not to be skewed. This information is then printed after all 10 folds have been completed. The classifier which obtained the highest mean accuracy is trained over the entire training set, and its statistics output via the \verb|classify| method.

The classifiers used can be split into four different categories, and will be discussed in the subsections below.

\subsubsection{Na{\"i}ve Bayes}
Na{\"i}ve Bayes methods are often used for text classification task. In this case three different methods were used and the results compared, as discussed in the evaluation section. Each will be described in turn, as well as any parameters chosen.

\verb|GaussianNB| (Gaussian Na{\"i}ve Bayes) assumes each class follows a Gaussian distribution. This is usually useful for continuous data, so it is expected that the TF-IDF vectorisation will be more relevant here.

\verb|MultinomialNB| (Multinomial Na{\"i}ve Bayes) uses a multinomial event model. This is expected to be relevant here in a multiclass scenario so should perform well. The default Laplascian parameter is used for smoothing and class prior probabilities are learnt. This should be used for discrete data, so should perform better for the count vectoriser.

\verb|BernoulliNB| (Bernoulli Na{\"i}ve Bayes) uses a Bernoulli event model assuming a binary vectorisation. Because of this a parameter is applied as a threshold of how to convert the vector into binary - 1 is given. The same smoothing is used and again class prior probabilities are learnt. This model is simpler than Multinomial and so may overfit less.

\subsubsection{Tree based}
Decision tree methods for classification can work well, however they have a high tendency to overfit. As this task uses sparse vectors of features it is likely that overfitting will be reduced as there should be a number of examples from each class with diverse features.

\verb|DecisionTreeClassifier| implements a decision tree classifier. Information gain is used for deciding whether to split attributes, as described in the lectures. The minimum samples per split and leaf were changed to 5 (greater than the default), as the amount of data is large and we want to avoid overfitting.

\verb|RandomForestClassifier| is an ensemble method using decision trees. The same parameters are used as for the decision tree classifier in order to be able to compare the two.

\subsubsection{Linear}
\verb|LinearRegression| implements simple least squares regression to find class boundaries. For this the data is centralised and normalised using flags. This is not expected to perform well as it can only find linear boundaries.

\verb|Perceptron| implements a linear perceptron, and also centralises the data. This is not expected to perform well as the data is not expected to be linearly separable into classes.

\verb|LinearSVC| was used as a scalable implementation of a Support Vector Machine (SVM) with a linear kernel. The data is again centralised using a flag. SVMs have often been shown to work well for classification tasks so this classifier should do well.

\verb|SVR| uses epsilon support vector regression, so allows some flexibility at the class boundaries. This also uses an 'rbf' kernel rather than a linear kernel so may perform differently to the linear SVC.

\subsubsection{Nearest neighbours}
\verb|KNeighborsClassifier| attempts to predict a class using an example's nearest neighbours and euclidean distance. The number of neighbours to check was increased to 10 in order to avoid classification based on outliers. 

\verb|NearestCentroid| represents classes via a representative or centroid.  The nearest centroid is used to classify an example.

\subsection{Metrics}
A number of different metrics will be used to evaluate the clustering algorithms. Most of these are available only for the SciLearn classifier algorithms, which will form the focus of most of the evaluation. All metrics take into account the multi-class scenario, using multi-class true and false positives and negatives (TP, FP, TN, FN).

The first of these, as used by both TextBlob and SciLearn, is the accuracy of the classifier. This metric describes where the label was predicted correctly for examples (TPs), and is normalised to a value between 0 and 1, where 1 is all examples being classified correctly. This metric will be used as described above during 10-fold cross validation to discover confidence intervals for the accuracy of each of the SciLearn classifiers.

The second metric to be used is the recall for each class over each of the 10 folds. This represents the formula $\frac{TP}{TP+FN}$, which informs about the classifier's ability to label positive examples correctly. Precision will also be reported for each class, which uses the formula $\frac{TP}{TP+FP}$ and represents a classifier's ability to label negative examples correctly. The f1 score represents the harmonic mean between precision and recall, giving a weighted average. 

The micro and macro averages for both precision and recall will also be reported, over all classes. The macro average averages the precision and recall values for each class, whereas the micro average calculates precision and recall using the average TP, FP and FN values for each class.

\section{Classifier Evaluation}
Each of the classifiers described above was trained over the training set, manipulated as described in the feature selection section, and then tested for the metrics described above. The results of this will be described in this section of the report.

\subsection{TextBlob classification}
Firstly the performance of the TextBlob classifiers will be described. For these there were not a lot of metrics easily available so only accuracy is displayed. These classifiers are interesting as they allow you to view the most informative features, as displayed below:

TODO

\subsection{SciLearn classification}
For SciLearn many more metrics were easily available, such as the f1 score and macro and micro recall. As 10-fold cross validation was also used we can create confidence intervals for the classifier accuracy. These are shown in the tables in the appendices.

TODO discuss results

TODO make t tests per classifier group, per all

TODO comment on which classes were easier to classify/overlap between classes?

TODO discuss performance of overall most successful

\section{Clustering}
Unsupervised clustering algorithms were also used to try to determine clusters relating to topics in the data. This task was completed using the SciLearn module, which includes various clustering algorithms. Feature selection was carried out as described above, using PCA to convert sparse vectors into dense vectors where required. In clustering algorithms the objective is to minimise inter-cluster distance whilst maximising intra-cluster distance.

\subsection{Algorithms}
The first algorithm used was the \verb|KMeans| algorithm, which takes as input the number of clusters to create. We know this as the number of unique topics present in the input data so it can be provided as a parameter. This algorithm iteratively moves the centroids of each cluster minimising the objective function. This algorithm works successfully on sparse vectors so PCA is not required.

The second clustering algorithm applied is \verb|Ward|, which is a hierarchical clustering method. The method recursively joins clusters which minimise inter-cluster distance, and then prunes the final dendrogram. This algorithm also allows the number of clusters as a parameter, and requires dense vectors as generated by PCA.

The third algorithm used is the \verb|DBSCAN| algorithm, which in contrast to \verb|KMeans| allows clusters to be any shape, so is capable of learning a much greater number of different boundaries. This algorithm aims to create high density clusters, with low density areas between them. The default parameters defining `density' are used. This algorithm requires dense vectors so PCA must be used. However it is likely that the clusters found won't closely correspond to the actual topics as DBSCAN is not aware of how many clusters should be formed.

The final algorithm used was \verb|GMM| (Gaussian Mixture Model), an implementation of the expectation-maximisation algorithm. This attempts to estimate the parameters of n Gaussian distributions, which in this case is the number of topics (for each topic we want one Gaussian distribution). The distribution each point is most likely to belong to corresponds to a `cluster' in the sense of clustering algorithms. This method is supervised, as it uses the actual topics in order to estimate the Gaussian parameters. This means it can also be tested over the test set, using the lewissplit in the same way as for the classification algorithms. However it requires dense vectors so PCA must be applied to both the training set and the test set. GMM has the ability to model any distribution of points around a cluster, however there will be outliers in the data and some topics may have very few examples.

\subsection{Metrics}
Five metrics will be used to evaluate the cluster algorithms in the section below. As we know the true labels of the data we can make comparisons between the clusters generated by the algorithms and the true topic clusters of the data.

The first three metrics used are related. Homogeneity is used to represent clusters containing members only of a single class, and the ideal value for this is 1. Completeness represents all members of a given topic being assigned to the same cluster, with an ideal value again of 1. The v-measure is the harmonic mean of these two metrics, and is equivalent to the normalised mutual information between the true and predicted topics. These metrics make no assumptions about the structure of clusters, however even a random assignment of examples to topics won't give 0, the worst possible score, when there are a large number of clusters, as in this example.

In this case, with a large number of clusters, it is recommended to use alternative metrics, such as the Adjusted Rand Index (ARI), which is given as the next metric. This measures the similarity between the clusters obtained and the true clustering of the data. If these match perfectly the score given by this metric is 1. Again this metric makes no assumptions about cluster structure.

The final metric used is the silhouette score, which does not require knowledge about the true labels of the data which aren't available in many real life clustering applications, however they are available in this case.  This indicates how well `defined' a cluster is using the following formula:

\begin{center}
$\frac{b-a}{max(a,b)}$
\end{center}

Where $a$ is the mean distance between an example and the other points in the same cluster, and $b$ is the mean distance between an example and all other points in the cluster that is the next closest after the one it was assigned to. A score of 1 indicates highly dense clustering, whereas -1 indicates an incorrect clustering. However the score is biased towards convex clusters, such as those generated by KMeans. In this case it is also a disadvantage that the true values are not used, however the density of a cluster still represents how closely the clusters found follow the concept of a true cluster.

The \verb|GMM| class is not in the same class hierarchy as the other clustering methods as it is supervised, so different metrics are available. This clustering algorithm will be evaluated using accuracy over both the training and test sets, and the results will not be very comparable with the other clustering methods.  

\subsection{Evaluation}

\section{Conclusion}


\section{Appendices}
TABLES
- confidence intervals per classifier, accuracy per fold (1 table per classifier type?), also macro micro averages per fold
- tables showing per class metrics, per classifier per fold: precision, recall, f-measure
- t test tables for comparing classifiers -> find best in category, find best overall with confidence
- tables for cluster metrics

\end{document}

